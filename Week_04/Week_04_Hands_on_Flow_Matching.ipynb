{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOZD1ip7F1tyTTBL/lShcxd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mechanics-Mechatronics-and-Robotics/CV-2026/blob/main/Week_04/Week_04_Hands_on_Flow_Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flow Matching Planner\n",
        "Generated with OpenAI ChatGPT 5.2.\n",
        "Feb. 08 2026"
      ],
      "metadata": {
        "id": "jpcyUgk1HQZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhUmuITlGy4B"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Week 04 Colab: Vision (2D) -> Flow Matching Planner -> Simple Controller\n",
        "# ==============================\n",
        "# This notebook is self-contained:\n",
        "#   1) Create synthetic 2D \"scene images\" with start/goal markers\n",
        "#   2) \"VL\" step (minimal): detect start/goal from the image (simple CV)\n",
        "#   3) Train a conditional Flow Matching model that generates 2D trajectories\n",
        "#   4) Sample trajectories via ODE rollout (Euler)\n",
        "#   5) (Optional) simple tracking controller demo\n",
        "#\n",
        "# Author: (your name)\n",
        "# ==============================\n",
        "\n",
        "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 || true\n",
        "!pip -q install numpy matplotlib tqdm opencv-python\n",
        "\n",
        "import math, random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Reproducibility ----------\n",
        "def seed_all(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_all(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ],
      "metadata": {
        "id": "dljD7t-oIsQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1) Synthetic \"scene images\" with start/goal\n",
        "# ==========================================\n",
        "# We render a 64x64 image with:\n",
        "#   - a red dot = start\n",
        "#   - a green dot = goal\n",
        "#\n",
        "# Then we \"detect\" those dots by simple color thresholding.\n",
        "#\n",
        "# This is a minimal stand-in for a VL/VLM module:\n",
        "# - Students see: perception -> geometry -> planning\n",
        "# - No heavy models required in Week 04\n",
        "\n",
        "H, W = 64, 64\n",
        "\n",
        "def render_scene(start_xy, goal_xy, radius=3):\n",
        "    \"\"\"\n",
        "    start_xy, goal_xy in normalized coordinates [-1,1]x[-1,1]\n",
        "    returns BGR uint8 image (H,W,3)\n",
        "    \"\"\"\n",
        "    img = np.zeros((H, W, 3), dtype=np.uint8)\n",
        "\n",
        "    def to_px(xy):\n",
        "        x, y = xy\n",
        "        # map [-1,1] -> [0,W-1], [0,H-1] with y up -> image y down\n",
        "        px = int((x + 1) * 0.5 * (W - 1))\n",
        "        py = int((1 - (y + 1) * 0.5) * (H - 1))\n",
        "        return px, py\n",
        "\n",
        "    sx, sy = to_px(start_xy)\n",
        "    gx, gy = to_px(goal_xy)\n",
        "\n",
        "    # BGR colors for OpenCV:\n",
        "    # start red, goal green\n",
        "    cv2.circle(img, (sx, sy), radius, (0, 0, 255), -1)\n",
        "    cv2.circle(img, (gx, gy), radius, (0, 255, 0), -1)\n",
        "\n",
        "    return img\n",
        "\n",
        "def detect_start_goal(img_bgr):\n",
        "    \"\"\"\n",
        "    Simple CV detection:\n",
        "      - threshold red and green,\n",
        "      - compute centroids,\n",
        "      - map back to normalized [-1,1] coordinates.\n",
        "    \"\"\"\n",
        "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # red mask (two ranges in HSV)\n",
        "    lower1 = np.array([0, 120, 50]); upper1 = np.array([10, 255, 255])\n",
        "    lower2 = np.array([170, 120, 50]); upper2 = np.array([180, 255, 255])\n",
        "    mask_red = cv2.inRange(hsv, lower1, upper1) | cv2.inRange(hsv, lower2, upper2)\n",
        "\n",
        "    # green mask\n",
        "    lower_g = np.array([35, 80, 50]); upper_g = np.array([85, 255, 255])\n",
        "    mask_green = cv2.inRange(hsv, lower_g, upper_g)\n",
        "\n",
        "    def centroid(mask):\n",
        "        ys, xs = np.where(mask > 0)\n",
        "        if len(xs) == 0:\n",
        "            return None\n",
        "        return float(xs.mean()), float(ys.mean())\n",
        "\n",
        "    c_red = centroid(mask_red)\n",
        "    c_green = centroid(mask_green)\n",
        "    if c_red is None or c_green is None:\n",
        "        return None\n",
        "\n",
        "    def to_norm(cx, cy):\n",
        "        # invert mapping used above\n",
        "        x = (cx / (W - 1)) * 2 - 1\n",
        "        y = -((cy / (H - 1)) * 2 - 1)\n",
        "        return np.array([x, y], dtype=np.float32)\n",
        "\n",
        "    start = to_norm(*c_red)\n",
        "    goal = to_norm(*c_green)\n",
        "    return start, goal\n",
        "\n",
        "# Quick visual sanity check\n",
        "start_xy = np.array([-0.6, -0.2], dtype=np.float32)\n",
        "goal_xy  = np.array([ 0.7,  0.5], dtype=np.float32)\n",
        "img = render_scene(start_xy, goal_xy)\n",
        "det = detect_start_goal(img)\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.title(f\"Detected: {det}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2gt1bX7wIun-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2) Trajectory dataset generator (2D paths)\n",
        "# ==========================================\n",
        "# We want a dataset of \"expert\" trajectories between (start, goal).\n",
        "# For Week 04 we generate *smooth curved paths* analytically.\n",
        "#\n",
        "# Each trajectory is a sequence of T waypoints in R^2.\n",
        "\n",
        "def make_smooth_trajectory(start, goal, T=32):\n",
        "    \"\"\"\n",
        "    start, goal: np arrays shape (2,) in [-1,1]\n",
        "    Create a smooth curve using a perpendicular sinusoidal bump.\n",
        "    \"\"\"\n",
        "    start = np.asarray(start, dtype=np.float32)\n",
        "    goal  = np.asarray(goal,  dtype=np.float32)\n",
        "    v = goal - start\n",
        "    norm = np.linalg.norm(v) + 1e-6\n",
        "    u = v / norm\n",
        "\n",
        "    # perpendicular direction\n",
        "    perp = np.array([-u[1], u[0]], dtype=np.float32)\n",
        "\n",
        "    # random curvature amplitude\n",
        "    amp = np.random.uniform(-0.35, 0.35) * min(1.0, norm)\n",
        "\n",
        "    ts = np.linspace(0.0, 1.0, T, dtype=np.float32)\n",
        "    pts = []\n",
        "    for t in ts:\n",
        "        base = (1 - t) * start + t * goal\n",
        "        bump = amp * math.sin(math.pi * t)  # 0 at ends, max mid\n",
        "        pts.append(base + bump * perp)\n",
        "    return np.stack(pts, axis=0)  # (T,2)\n",
        "\n",
        "# visualize a few random trajectories\n",
        "T = 32\n",
        "plt.figure(figsize=(5,5))\n",
        "for _ in range(12):\n",
        "    s = np.random.uniform(-0.9, 0.9, size=(2,)).astype(np.float32)\n",
        "    g = np.random.uniform(-0.9, 0.9, size=(2,)).astype(np.float32)\n",
        "    tr = make_smooth_trajectory(s, g, T=T)\n",
        "    plt.plot(tr[:,0], tr[:,1], alpha=0.6)\n",
        "    plt.scatter([s[0], g[0]],[s[1], g[1]], s=10)\n",
        "plt.xlim(-1,1); plt.ylim(-1,1); plt.gca().set_aspect(\"equal\", \"box\")\n",
        "plt.title(\"Synthetic expert trajectories\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rNLblJEzJZjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# 3) Conditional Flow Matching for trajectories (core section)\n",
        "# ===========================================================\n",
        "# We model the *entire trajectory* as a single vector in R^(2T):\n",
        "#   x in R^(2T)  represents concatenated waypoints.\n",
        "#\n",
        "# We train an ODE vector field v_theta(x,t | c) that transports\n",
        "# noise x0 -> data x1 (expert trajectory), conditioned on c = (start,goal).\n",
        "#\n",
        "# Training objective (simple linear coupling):\n",
        "#   x(t) = (1-t)*x0 + t*x1\n",
        "#   v*(t) = x1 - x0\n",
        "#   L = || v_theta(x(t),t,c) - v*(t) ||^2\n",
        "\n",
        "T = 32\n",
        "D = 2*T  # trajectory vector dimension\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, t):\n",
        "        # t: (B,1)\n",
        "        half = self.dim // 2\n",
        "        freqs = torch.exp(\n",
        "            torch.linspace(math.log(1.0), math.log(1000.0), half, device=t.device)\n",
        "        )  # (half,)\n",
        "        ang = t * freqs[None, :] * 2*math.pi\n",
        "        emb = torch.cat([torch.sin(ang), torch.cos(ang)], dim=-1)\n",
        "        if self.dim % 2 == 1:\n",
        "            emb = F.pad(emb, (0,1))\n",
        "        return self.mlp(emb)\n",
        "\n",
        "class FlowField(nn.Module):\n",
        "    def __init__(self, traj_dim, cond_dim=4, time_dim=64, hidden=512):\n",
        "        super().__init__()\n",
        "        self.time = TimeEmbedding(time_dim)\n",
        "        in_dim = traj_dim + cond_dim + time_dim\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden, traj_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, cond):\n",
        "        # x: (B, D), t: (B,1), cond: (B,4)\n",
        "        te = self.time(t)  # (B,time_dim)\n",
        "        h = torch.cat([x, cond, te], dim=-1)\n",
        "        return self.net(h)\n",
        "\n",
        "model = FlowField(traj_dim=D, cond_dim=4, time_dim=64, hidden=512).to(device)\n",
        "sum(p.numel() for p in model.parameters()) / 1e6\n"
      ],
      "metadata": {
        "id": "MRtb_wbRJgg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Training data sampler\n",
        "# ---------------------------\n",
        "def sample_batch(batch_size, T=32):\n",
        "    # sample start/goal in [-0.9,0.9]\n",
        "    s = np.random.uniform(-0.9, 0.9, size=(batch_size, 2)).astype(np.float32)\n",
        "    g = np.random.uniform(-0.9, 0.9, size=(batch_size, 2)).astype(np.float32)\n",
        "\n",
        "    trajs = np.stack([make_smooth_trajectory(s[i], g[i], T=T) for i in range(batch_size)], axis=0)  # (B,T,2)\n",
        "    x1 = trajs.reshape(batch_size, -1)  # (B,2T)\n",
        "    cond = np.concatenate([s, g], axis=-1)  # (B,4)\n",
        "    return torch.tensor(x1), torch.tensor(cond)\n",
        "\n",
        "# quick check shapes\n",
        "x1, cond = sample_batch(8, T=T)\n",
        "x1.shape, cond.shape\n"
      ],
      "metadata": {
        "id": "H5y1cHn2Jo3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Flow Matching training loop\n",
        "# ---------------------------\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
        "\n",
        "def train_flow_matching(steps=6000, batch_size=256, log_every=200):\n",
        "    model.train()\n",
        "    pbar = tqdm(range(steps))\n",
        "    for step in pbar:\n",
        "        x1, cond = sample_batch(batch_size, T=T)\n",
        "        x1 = x1.to(device)\n",
        "        cond = cond.to(device)\n",
        "\n",
        "        # base noise (same dimension as trajectory vector)\n",
        "        x0 = torch.randn_like(x1)\n",
        "\n",
        "        # sample time\n",
        "        t = torch.rand(batch_size, 1, device=device)\n",
        "\n",
        "        # linear interpolation\n",
        "        xt = (1 - t) * x0 + t * x1\n",
        "\n",
        "        # target velocity\n",
        "        v_star = x1 - x0\n",
        "\n",
        "        # predicted velocity\n",
        "        v_hat = model(xt, t, cond)\n",
        "\n",
        "        loss = F.mse_loss(v_hat, v_star)\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        if (step + 1) % log_every == 0:\n",
        "            pbar.set_description(f\"step {step+1}/{steps} | loss {loss.item():.4f}\")\n",
        "\n",
        "train_flow_matching(steps=4000, batch_size=256, log_every=200)\n"
      ],
      "metadata": {
        "id": "i9lNoytTJwMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4) Sampling: roll out ODE with Euler steps\n",
        "# ==========================================\n",
        "@torch.no_grad()\n",
        "def sample_trajectory(cond, n_steps=50):\n",
        "    \"\"\"\n",
        "    cond: (B,4) tensor on device\n",
        "    Returns: (B,T,2) trajectory points\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    B = cond.shape[0]\n",
        "    x = torch.randn(B, D, device=device)  # x(0) ~ N(0,I)\n",
        "    t0, t1 = 0.0, 1.0\n",
        "    dt = (t1 - t0) / n_steps\n",
        "\n",
        "    t = torch.full((B,1), t0, device=device)\n",
        "    for _ in range(n_steps):\n",
        "        v = model(x, t, cond)\n",
        "        x = x + dt * v\n",
        "        t = t + dt\n",
        "\n",
        "    traj = x.reshape(B, T, 2).detach().cpu().numpy()\n",
        "    return traj\n",
        "\n",
        "# Visualize samples vs \"expert\" for random conditions\n",
        "B = 6\n",
        "x1_gt, cond = sample_batch(B, T=T)\n",
        "cond = cond.to(device)\n",
        "\n",
        "traj_pred = sample_trajectory(cond, n_steps=60)\n",
        "traj_gt = x1_gt.reshape(B, T, 2).numpy()\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "for i in range(B):\n",
        "    s = cond[i,:2].detach().cpu().numpy()\n",
        "    g = cond[i,2:].detach().cpu().numpy()\n",
        "    plt.plot(traj_gt[i,:,0], traj_gt[i,:,1], linestyle=\"--\", alpha=0.7, label=\"GT\" if i==0 else None)\n",
        "    plt.plot(traj_pred[i,:,0], traj_pred[i,:,1], alpha=0.9, label=\"FM\" if i==0 else None)\n",
        "    plt.scatter([s[0], g[0]], [s[1], g[1]], s=30)\n",
        "plt.xlim(-1,1); plt.ylim(-1,1); plt.gca().set_aspect(\"equal\", \"box\")\n",
        "plt.legend()\n",
        "plt.title(\"Flow Matching trajectories (solid) vs expert (dashed)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GHQD3fbDLUua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# 5) End-to-end demo: Image -> detect -> FM -> visualize\n",
        "# ======================================================\n",
        "@torch.no_grad()\n",
        "def plan_from_image(img_bgr, n_steps=60):\n",
        "    det = detect_start_goal(img_bgr)\n",
        "    if det is None:\n",
        "        raise RuntimeError(\"Could not detect start/goal.\")\n",
        "    start, goal = det\n",
        "    cond = torch.tensor(np.concatenate([start, goal])[None, :], dtype=torch.float32, device=device)\n",
        "    traj = sample_trajectory(cond, n_steps=n_steps)[0]  # (T,2)\n",
        "    return start, goal, traj\n",
        "\n",
        "# Create a random scene, plan, and overlay trajectory on image\n",
        "start_xy = np.random.uniform(-0.9, 0.9, size=(2,)).astype(np.float32)\n",
        "goal_xy  = np.random.uniform(-0.9, 0.9, size=(2,)).astype(np.float32)\n",
        "img = render_scene(start_xy, goal_xy)\n",
        "\n",
        "start_d, goal_d, traj = plan_from_image(img, n_steps=60)\n",
        "\n",
        "# draw trajectory on image\n",
        "img_vis = img.copy()\n",
        "\n",
        "def norm_to_px(xy):\n",
        "    x, y = float(xy[0]), float(xy[1])\n",
        "    px = int((x + 1) * 0.5 * (W - 1))\n",
        "    py = int((1 - (y + 1) * 0.5) * (H - 1))\n",
        "    return px, py\n",
        "\n",
        "for k in range(T-1):\n",
        "    p1 = norm_to_px(traj[k])\n",
        "    p2 = norm_to_px(traj[k+1])\n",
        "    cv2.line(img_vis, p1, p2, (255, 255, 255), 1)  # white polyline\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(cv2.cvtColor(img_vis, cv2.COLOR_BGR2RGB))\n",
        "plt.title(f\"Detected start={start_d.round(2)}, goal={goal_d.round(2)}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.plot(traj[:,0], traj[:,1])\n",
        "plt.scatter([start_d[0], goal_d[0]], [start_d[1], goal_d[1]], s=60)\n",
        "plt.xlim(-1,1); plt.ylim(-1,1); plt.gca().set_aspect(\"equal\",\"box\")\n",
        "plt.title(\"Planned trajectory in normalized coords\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AWU3Wk7jLhyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# 6) (Optional) Simple tracker controller demo (no RL)\n",
        "# ======================================================\n",
        "# We simulate a point-mass that tries to follow planned waypoints\n",
        "# with a proportional controller + injected disturbances.\n",
        "\n",
        "def track_trajectory(traj, K=0.8, noise_std=0.02):\n",
        "    \"\"\"\n",
        "    traj: (T,2) desired waypoints\n",
        "    returns executed positions (T,2)\n",
        "    \"\"\"\n",
        "    x = traj[0].copy()\n",
        "    xs = [x.copy()]\n",
        "    for t in range(1, len(traj)):\n",
        "        target = traj[t]\n",
        "        u = K * (target - x)\n",
        "        # disturbance\n",
        "        x = x + u + np.random.randn(2).astype(np.float32) * noise_std\n",
        "        xs.append(x.copy())\n",
        "    return np.stack(xs, axis=0)\n",
        "\n",
        "exec_traj = track_trajectory(traj, K=0.7, noise_std=0.03)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(traj[:,0], traj[:,1], label=\"planned (FM)\")\n",
        "plt.plot(exec_traj[:,0], exec_traj[:,1], label=\"executed (controller)\", linestyle=\"--\")\n",
        "plt.scatter([traj[0,0], traj[-1,0]],[traj[0,1], traj[-1,1]], s=80)\n",
        "plt.xlim(-1,1); plt.ylim(-1,1); plt.gca().set_aspect(\"equal\",\"box\")\n",
        "plt.legend()\n",
        "plt.title(\"Tracking demo: why you need a controller (and later RL/MPC)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ugRxAMmmLr_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Notes for your lecture/lab\n",
        "# =========================\n",
        "# - What students should observe:\n",
        "#   1) FM produces smooth, plausible trajectories conditioned on (start, goal)\n",
        "#   2) The controller can deviate under disturbances\n",
        "#   3) This motivates \"RL as a tracker\" in Week 05/06\n",
        "#\n",
        "# - Extensions you can assign:\n",
        "#   A) Add obstacles in the image and encode them in cond.\n",
        "#   B) Train FM to go around obstacles (change expert generator).\n",
        "#   C) Replace simple controller with MPC or (later) RL.\n",
        "#\n",
        "# Done.\n"
      ],
      "metadata": {
        "id": "g9rHDIoWLxvP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}